
# 2024-08-26 11:27:57.771356
+Hello, on va créer une application python / streamlit assez simple. Le but est de permettre de rentrer une URL d'une page Wikipédia et d'avoir une voix qui nous lit l'article.
+On va également demander a une IA de réécrire légèrement l'article afin qu'il soit plus agréable à la lecture. On va aussi permettre à l'utilisateur de choisir le type de voix
+On va découper l'application comme il suit :
+1) Une API flask qui fera toute la partie traitement et sera autonome (dans le but pouvoir al déployée seule et d'utiliser l'API avec une autre interface.
+L'API sera limité a un seul endpoint, qui prendra en entrée l'URL de la page wiki et différents paramètre que l'on ajoutera plus tard, et répond par le fichier audio.
+2) Une interface simple en streamlit.
+
+On va commencer par créer la structure du projet et de l'API

# 2024-08-26 11:28:26.730328
+Oui !

# 2024-08-26 11:29:08.833916
+Voilà !

# 2024-08-26 12:03:10.661819
+/home/gmartin/.pyenv/versions/aiserv2/lib/python3.11/site-packages/bark/generation.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
+  checkpoint = torch.load(ckpt_path, map_location=device)
+/home/gmartin/.pyenv/versions/aiserv2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
+  warnings.warn(
+/home/gmartin/.pyenv/versions/aiserv2/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
+  WeightNorm.apply(module, name, dim)
+ * Serving Flask app 'app' (lazy loading)
+ * Environment: production
+   WARNING: This is a development server. Do not use it in a production deployment.
+   Use a production WSGI server instead.
+ * Debug mode: on
+WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
+ * Running on http://127.0.0.1:5000
+Press CTRL+C to quit
+ * Restarting with watchdog (inotify)
+No GPU being used. Careful, inference might be very slow!
+/home/gmartin/.pyenv/versions/aiserv2/lib/python3.11/site-packages/bark/generation.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
+  checkpoint = torch.load(ckpt_path, map_location=device)
+/home/gmartin/.pyenv/versions/aiserv2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
+  warnings.warn(
+[1]    60634 killed     python -m api.app

# 2024-08-26 12:05:11.912866
+Traceback (most recent call last):
+  File "<frozen runpy>", line 198, in _run_module_as_main
+  File "<frozen runpy>", line 88, in _run_code
+  File "/home/gmartin/worspace/wiki-narrative/api/app.py", line 74, in <module>
+    app.run(debug=True)
+  File "/home/gmartin/.pyenv/versions/aiserv2/lib/python3.11/site-packages/flask/app.py", line 920, in run
+    run_simple(t.cast(str, host), port, self, **options)
+  File "/home/gmartin/.pyenv/versions/aiserv2/lib/python3.11/site-packages/werkzeug/serving.py", line 1090, in run_simple
+    run_with_reloader(
+  File "/home/gmartin/.pyenv/versions/aiserv2/lib/python3.11/site-packages/werkzeug/_reloader.py", line 434, in run_with_reloader
+    ensure_echo_on()
+  File "/home/gmartin/.pyenv/versions/aiserv2/lib/python3.11/site-packages/werkzeug/_reloader.py", line 414, in ensure_echo_on
+    termios.tcsetattr(sys.stdin, termios.TCSANOW, attributes)
+termios.error: (5, 'Input/output error')

# 2024-08-26 12:08:22.079989
+Traceback (most recent call last):
+  File "/home/gmartin/.pyenv/versions/aiserv2/lib/python3.11/site-packages/flask/app.py", line 2077, in wsgi_app
+    response = self.full_dispatch_request()
+               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+  File "/home/gmartin/.pyenv/versions/aiserv2/lib/python3.11/site-packages/flask/app.py", line 1525, in full_dispatch_request
+    rv = self.handle_user_exception(e)
+         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+  File "/home/gmartin/.pyenv/versions/aiserv2/lib/python3.11/site-packages/flask/app.py", line 1523, in full_dispatch_request
+    rv = self.dispatch_request()
+         ^^^^^^^^^^^^^^^^^^^^^^^
+  File "/home/gmartin/.pyenv/versions/aiserv2/lib/python3.11/site-packages/flask/app.py", line 1509, in dispatch_request
+    return self.ensure_sync(self.view_functions[rule.endpoint])(**req.view_args)
+           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+  File "/home/gmartin/worspace/wiki-narrative/api/app.py", line 61, in generate_audio
+    audio_file = generate_audio_file(content)
+                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+  File "/home/gmartin/worspace/wiki-narrative/api/app.py", line 27, in generate_audio_file
+    audio_array = generate_audio(text, device=device)
+                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+TypeError: generate_audio() got an unexpected keyword argument 'device'

# 2024-08-26 14:17:16.597303
+Il y a deux erreur, le nombre de token trop grand, et le fait que l'on ne renvoie pas le bon fichier : 
+
+Token indices sequence length is longer than the specified maximum sequence length for this model (20301 > 512). Running this sequence through the model will result in indexing errors
+warning, text too long, lopping of last 98.7%
+100%|████████████████████████████████████████████████████████| 698/698 [00:54<00:00, 12.84it/s]
+100%|██████████████████████████████████████████████████████████| 35/35 [05:03<00:00,  8.67s/it]
+Audio generated : /tmp/tmp16ihlivt.wav
+[2024-08-26 12:17:31,966] ERROR in app: Exception on /generate_audio [POST]
+Traceback (most recent call last):
+  File "/home/gmartin/.pyenv/versions/aiserv2/lib/python3.11/site-packages/flask/app.py", line 2077, in wsgi_app
+    response = self.full_dispatch_request()
+               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+  File "/home/gmartin/.pyenv/versions/aiserv2/lib/python3.11/site-packages/flask/app.py", line 1526, in full_dispatch_request
+    return self.finalize_request(rv)
+           ^^^^^^^^^^^^^^^^^^^^^^^^^
+  File "/home/gmartin/.pyenv/versions/aiserv2/lib/python3.11/site-packages/flask/app.py", line 1547, in finalize_request
+    response = self.process_response(response)
+               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+  File "/home/gmartin/.pyenv/versions/aiserv2/lib/python3.11/site-packages/flask/app.py", line 1889, in process_response
+    response = self.ensure_sync(func)(response)
+               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+  File "/home/gmartin/worspace/wiki-narrative/api/app.py", line 73, in cleanup
+    os.remove(filename)
+FileNotFoundError: [Errno 2] No such file or directory: 'wiki_audio.wav'

# 2024-08-26 14:18:05.787515
+Je pense que plutôt que limiter, il faudrait découper l'article en chunk, traiter tout les chunks, puis les funionner

# 2024-08-26 16:02:47.881946
+Pour le chunck, peut être que l'on pourrait utiliser nltk (Natural language toolkit) to split into sentances. 

# 2024-08-26 18:02:10.571305
+  File "<frozen runpy>", line 198, in _run_module_as_main
+  File "<frozen runpy>", line 88, in _run_code
+  File "/home/orolol/workspace/tellmepedia/api/app.py", line 1, in <module>
+    from flask import Flask, request, jsonify, send_file
+  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
+  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
+  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
+  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
+  File "<frozen importlib._bootstrap_external>", line 1073, in get_code
+  File "<frozen importlib._bootstrap_external>", line 1131, in get_data
+OSError: [Errno 5] Input/output error

# 2024-08-26 18:04:21.642286
+voila

# 2024-08-27 10:18:40.207226
+Please handle the language in the wikipedia lib use.
